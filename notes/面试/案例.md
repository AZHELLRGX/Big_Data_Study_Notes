 一个文件只有一行，但是这行有100G大小，mr会不会切分，我们应该怎么解决

> 1、首先需要了解HDFS的Block，以及MapReduce的Split的概念；Block是物理划分，默认单个块是128M，HDFS不会按照换行符对文件进行切分，所以一个100G的并且支持分块的文件一定会被分割存储；而Split则是逻辑切分概念，默认使用的换行符进行切分，当然我们依旧可以使用如下配置设置需要的换行符
>
>  conf.set("textinputformat.record.delimiter", "|-|\n");   
>
> 2、MapReduce将HDFS的文件视为一个整体，虽然物理存储上是多个Block，但是这对上层数据读取是透明的
>
> 3、需要注意的是有些文件不支持逻辑切片，例如如果采用了SequenceFile，而且不是使用Lzo或是BZip进行压缩的，那么就很可能不支持逻辑切片，这就导致严重的数据倾斜
>
> 4、参考文章： [Hadoop基础-MapReduce的工作原理第二弹 - 尹正杰 - 博客园 (cnblogs.com)](https://www.cnblogs.com/yinzhengjie/p/9189858.html) 
>
>  [Hadoop切分纯文本时对某一行跨两个分片这种情况的处理 - 简书 (jianshu.com)](https://www.jianshu.com/p/fe37603b4741) 

