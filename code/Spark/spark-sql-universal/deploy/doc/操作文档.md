### 程序目录结构

如图1-1所示，通用Spark程序共分为5个主要目录：

- `bin`   程序启动的运行脚本所在目录
- `conf` 程序的配置文件所在目录，其下包含properties和xml两个子目录；具体配置文件相关将在后文讲解
- `doc` 程序文档手册所在目录
- `lib` 程序主体以及插件所在目录
- `log` 程序日志所在目录

![image-20210726174022852](%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3.assets/image-20210726174022852.png)

<center style="color:#ffff66;">图1-1 通用SparkSQL标准目录结构</center>

![image-20210909091229220](%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3.assets/image-20210909091229220.png)

<center style="color:#ffff66;">图1-2 通用SparkSQL程序树状目录结构</center>

### 配置文件说明

#### Properties配置文件

##### core.properties

用来配置程序相关的核心配置，例如程序名称，是否为调试模式等信息

> - app-name   【无默认值】【缺省配置为TESTING】程序提交到yarn的时候，在yarn的标识信息
>
> ![image-20210727092719908](%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3.assets/image-20210727092719908.png)
>
> - debug 【是否调试】【缺省配置为false】一般线上运行建议改为false，否则程序会以LOCAL模式在程序所在接口机运行

##### database.properties

用来配置数据库连接等信息

> 格式如下：
>
> `<NAME>=<DATABASE>-<USER>-<PASSWORD>-<DBTYPE>`
>
> 配置信息以英文符号`-`分割
>
> 示例如下：
>
> ```properties
> # 配置示例参考：jdbc:mysql://192.168.1.71:3306/test2-dtauser-dtauser-mysql
> # 尽量不要使用地市库id作为数据库id
> BEIJING=vkKh0wl8SaiChWar0u6h5oFCPIffwP6RqB7dyhfHlzxJiJLtT9Wk4BHkCuzSVts708x0z1v3tckkq+cw5Gw3UkakE1xjknhkmI+6h5zNTY6Q7Kb2XTFYvA2YTt1fXKAN
> MAIN=vkKh0wl8SaiChWar0u6h5oFCPIffwP6RqB7dyhfHlzxJiJLtT9Wk4BHkCuzSVts708x0z1v3tcliPJxfNd8RnD5k2cZxFVQOk5B9HSRaKLGtErQMo1HK8w==
> ```
>
> 目前仅支持MYSQL以及SQLSERVER数据库连接配置
>
> DBTYPE属性分别为mysql以及mssql；此为唯一配置，没有其他别名

##### udf.properties【过时】

UDF函数维护还是可以交给大数据底层开发人员，因为scala或者Java对于SQL编写人员还是比较陌生的，UDF统一维护在此程序中其实也是一种比较好的方案；所以目前的版本中不支持

目前程序内部支持以下三个UDF函数

>PointExtendFunction（判断点是否在物业点轮廓内）(poly: String, longitude: Double, latitude: Double, radius: Double)
>
>PolyGridListFunction（物业点栅格化）(poly: String, gridSize: Integer) 
>
>PointDistanceFunction（计算两个点的经纬度）(lon1: Double, lat1: Double, lon2: Double, lat2: Double)

#### XM配置文件

XML配置标签目前分为四个，涵盖了数据读取、数据交互、数据输出三个方面，以下从XML标签、属性以及分类三个方面详细讲解

##### 标签

```xml
<!-- 目前主要包含三类标签，以下是示例 -->

<!-- 将缓存相关配置设置为cache标签，统一使用MEMORY_AND_DISK缓存模式 -->
<cache>
   	<!-- 当前配置文件配置内哪些数据生成后需要缓存 -->
    <table>aoiDbTemp</table>
    <table>addressTemp</table>
</cache>

<!-- HDFS数据字段解析映射配置 -->
<fieldMap name="simplifyDataMapping">
    <!-- index表示字段序列【从0开始】，type为字段类型【统一参考Hive类型】，标签Value为映射为table后的字段名称-->
    <field index="0" type="int">cityId</field>
    <field index="1" type="String">imsi</field>
    <field index="6" type="bigint">eci</field>
</fieldMap>

<!-- SQL标签，通用配置，根据不用的属性配置可以完成不同的任务 -->
<sql dataPath="/usr/rgx/shanxi/resident_config/simplify" delimiter="\t"
     fieldMap="simplifyDataMapping" sparkMapName="simplify_temp" registerTableName="simplify">
    <!-- 此处SQL可以为空,为空则Spark注册表名【registerTableName】不生效 -->
    SELECT * FROM simplify_temp WHERE cityId = 6101
</sql>
```

##### SQL标签属性统一说明

> dbId【数据库连接信息，对应database.properties中的数据库连接配置】
> dbTableName【读取、输出数据的时候指定的数据库中实际表名称；支持通配符】
> dataPath【HDFS数据读取与输出的时候指定的绝对路径；支持通配符】
> delimiter【HDFS数据分隔符，默认值为制表符`\t`；在从HDFS读取数据与写出数据的时候生效】
> fieldMap【HDFS字段解析配置名】
> sparkMapName【1、源数据读取完成后映射的SparkSQL表名称；2、数据输出的时候指定输出数据在Spark的对应名称；3、如果此属性未指定，则默认使用dbTableName属性值，但务必注意中文名称问题】
> registerTableName【存在SQL执行语句的标签，SQL执行完成后新注册的Spark表名称，没有SQL语句可以无需配置】
> method【数据写出的方式，是覆写(overwrite)还是追加(append)】
> cityDistribution【是否需要从多地市读取数据或者是否需要写出数据分发到地市库】
> multiDays【从数据库或者HDFS读取多天数据，自动从当前日期往前扫描的范围，1就是表示往前扫描一天，加上当天数据就是两个天表】

##### 关于通配符

```xml
<!-- 日期和厂商通配符，例如`tb_create_test_${provider}_${date}` -->
<sql dbId="DB1" dbTableName="tb_create_test_${provider}_${date}" registerTableName="create_test"></sql>
<!-- 如果是多天模式，可以使用${multiDate}作为日期通配符 -->
<sql dbId="DB1" dbTableName="tb_create_test_${provider}_${multiDate}" registerTableName="create_test" multiDays="2"></sql>
```

##### sourceDataFromDB.xml

```xml
<!-- 配置模版 -->
<?xml version="1.0" encoding="UTF-8"?>

<readFromDB>

    <cache>
        <table>aoiDbTemp</table>
        <table>addressTemp</table>
    </cache>

    <sql dbId="DB1" dbTableName="aoi_db" registerTableName="aoiDbTemp">
        <!-- 此处SQL可以为空,为空则Spark注册表名不生效 -->
        SELECT * FROM aoi_db WHERE areaId = 336205
    </sql>

    <sql dbId="DB2" dbTableName="address" sparkMapName="addressFromDB" registerTableName="addressTemp">
        SELECT * FROM addressFromDB WHERE id = 1
    </sql>

    <sql dbId="DB1" dbTableName="tb_aaa_zc打点" sparkMapName="tb_aaa_zc_point" registerTableName="tb_aaa_zc_point_temp">
        SELECT * FROM tb_aaa_zc_point WHERE `经度` = 1089058180
    </sql>

    <!-- sparkMapName不指定，就默认使用dbTableName名称 -->
    <sql dbId="DB1" dbTableName="tb_cfg_cell_lte" registerTableName="tb_cfg_cell_xian">
        SELECT eci,`小区名称` FROM tb_cfg_cell_lte
    </sql>
</readFromDB>
```

##### sourceDataFromDFS.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>

<readFromHDFS>
    <!-- HDFS数据字段映射示例，name属性可以配置在sql标签的fieldMap属性中 -->
    <fieldMap name="simplifyDataMapping">
        <field index="0" type="int">cityId</field>
        <field index="1" type="String">imsi</field>
        <field index="6" type="bigint">eci</field>
    </fieldMap>

    <!-- 如果是yarn集群模型运行，数据路径中的HDFS集群地址【hdfs://192.168.2.110:8020】可以不写 -->
    <sql dataPath="hdfs://192.168.2.110:8020/usr/rgx/shanxi/resident_config/simplify" delimiter="\t"
         fieldMap="simplifyDataMapping" sparkMapName="simplify_temp" registerTableName="simplify">
        <!-- 此处SQL可以为空,为空则Spark注册表名不生效 -->
        SELECT * FROM simplify_temp WHERE cityId = 6101
    </sql>
</readFromHDFS>
```

##### dataProcessing.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<prosssing>
    <!-- 数据处理部分其实只需要一个sql以及注册表 -->
    <!-- 数据按照SQL标签的默认顺序依次执行 -->
    <sql registerTableName="simplify_relate_cell">
        SELECT a.cityId,a.imsi,a.eci,b.`小区名称` FROM simplify a
        LEFT JOIN tb_cfg_cell_xian b
        ON a.eci = b.eci
    </sql>
</prosssing>
```

##### dataOutPut.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<output>

    <!-- 数据写出到DB -->
    <!-- 写出模式包含：overwrite、append -->
    <!-- sparkMapName与sql语句至少存在一个，会优先写出sql的执行结果，其次才会选择输出sparkMapName对应的数据；但还是建议将执行逻辑放置在dataProcessing标签，作为交互逻辑，输出部分只需要配置程序名称即可-->
    <sql dbId="DB1" method="overwrite" dbTableName="simplify_relate_cell" sparkMapName="">
        SELECT cityId,imsi,eci,`小区名称` as eciName FROM simplify_relate_cell limit 4
    </sql>

    <!-- 数据写出到DFS文件系统 -->
    <sql dataPath="/usr/rgx/shanxi/resident_config/simplify_relate_cell"
         sparkMapName="simplify_relate_cell" delimiter="\t">
    </sql>

</output>
```

### 脚本说明

目前存在两个脚本，控制脚本`start.sh`以及运行脚本`run.sh`

#### start.sh

```shell
# 必须输入应用名称，程序名称是查找配置的关键信息
#appName=$1
# 可以接收输入的日期参数[例如：20210727]
#date=$2
# 可以输入的厂商参数[yd、lt、dx]
#provider=$3
queue_name=default
# driver内存设置
driver_memory=1g
# 执行器数量设置
num_executors=20
# 每个执行器的内存设置
executor_memory=2g
# 每个执行器核数设置
executor_cores=2
# 计算并行度设置
parallelism=50

shell_path="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
echo $shell_path
log_path=$shell_path/../log

# 如果脚本输入了日期和厂商参数，他们一并也会提交到程序中去
if [ $# != 3 ] ; then
	time=$(date "+%Y%m%d")
	current_day=`echo $time |cut -b 3-8`
	nohup sh $shell_path/run.sh $queue_name $driver_memory $num_executors $executor_memory $executor_cores $parallelism $1 > $log_path/$1/$current_day.log 2>&1 &
	tailf $log_path/$1/$current_day.log
else
	nohup sh $shell_path/run.sh $queue_name $driver_memory $num_executors $executor_memory $executor_cores $parallelism $1 $2 $3 > $log_path/$1/$2.log 2>&1 &
	tailf $log_path/$1/$2.log
fi
# 程序会自动tailf查看日志，可以随时使用ctrl + c结束
```

#### Spark配置参数说明

```properties
# 队列设置
queue_name=default
# driver内存设置
driver_memory=1g
# 执行器数量设置
num_executors=20
# 每个执行器的内存设置
executor_memory=2g
# 每个执行器核数设置
executor_cores=2
# 计算并行度设置
parallelism=50

# 以上参数有机会会单独详细解释，以及他们在调优中发挥的作用
```

#### run.sh

```shell
# 获取当前脚本的绝对路径
shell_path="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
echo $shell_path
jar_path="${shell_path}/../lib/core/spark-sql-universal-1.2.0.jar"
echo $jar_path

# 接收输入参数
queue_name=$1
driver_memory=$2
num_executors=$3
executor_memory=$4
executor_cores=$5
parallelism=$6


if [ $# == 7 ] ; then
	spark-submit --queue $queue_name --master yarn --driver-memory $driver_memory --num-executors $num_executors --executor-memory $executor_memory --executor-cores $executor_cores --conf spark.default.parallelism=$parallelism --jars $shell_path/../lib/third/sqljdbc4-4.0.jar,$shell_path/../lib/third/mysql-connector-java-5.1.47.jar --class cn.mastercom.bigdata.main.MainJob $jar_path $shell_path $7
else
	spark-submit --queue $queue_name --master yarn --driver-memory $driver_memory --num-executors $num_executors --executor-memory $executor_memory --executor-cores $executor_cores --conf spark.default.parallelism=$parallelism --jars $shell_path/../lib/third/sqljdbc4-4.0.jar,$shell_path/../lib/third/mysql-connector-java-5.1.47.jar --class cn.mastercom.bigdata.main.MainJob $jar_path $shell_path $7 $8 $9
fi
```

#### 启动脚本

```shell
# 建议最好在程序的根目录执行脚本
sh bin/start.sh TESTING # 只有一个APP名称参数，默认使用当天日期，厂商默认是yd
sh bin/start.sh TESTING 210907 yd  # 传入APP名称、日期和厂商参数，日期是6位数格式
```

#### 测试

在`192.168.2.101`机器上的`/home/hmaster/rgx/sparksql-universal`有一个完整的测试程序，你可以直接运行它们，观察程序执行过程和效果

在执行脚本之前，请先在`192.168.2.101`机器上清理一下HDFS数据目录

```shell
 hadoop fs -rm -r -skipTrash /usr/rgx/resident/tb_mr_user_location_group_yd_dd_210907
```

如果在使用过程中有建议或者想法可以直接与开发人员沟通，我们会逐渐增加新的特性，方便你的使用

